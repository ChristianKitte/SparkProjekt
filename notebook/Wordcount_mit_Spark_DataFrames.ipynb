{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Wordcount_mit_Spark_DataFrames",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNttuMq1qDODuqi9LFEoAyb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChristianKitte/SparkProjekt/blob/main/notebook/Wordcount_mit_Spark_DataFrames.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJtej7pTMlmr"
      },
      "source": [
        "# Vorbereitung des Notebooks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VREA6gqNAMs8",
        "outputId": "77f8ca2a-8010-4e33-85de-3ebda215c742"
      },
      "source": [
        "# Installation  von Java\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "print(\"Java ist installiert...\")\n",
        "\n",
        "# Download und Entpacken von Spark (Versionsnummer anpassen!)\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.0-bin-hadoop3.2.tgz\n",
        "\n",
        "print(\"Spark ist verfügbar...\")\n",
        "\n",
        "# Setzen der Systemvariablen für Java und Spark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.0-bin-hadoop3.2\"\n",
        "\n",
        "print(\"Umgebungsvariablen sind gesetzt...\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Java ist installiert...\n",
            "Spark ist verfügbar...\n",
            "Umgebungsvariablen sind gesetzt...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWPQDeb8bl9M",
        "outputId": "96e81ef9-6529-417d-8ef4-37fb9e9f2ed1"
      },
      "source": [
        "# Installation von findspark und pyspark\n",
        "\n",
        "!pip install findspark\n",
        "print(\"FindSpark wurde installiert...\")\n",
        "\n",
        "!pip install pyspark\n",
        "print(\"PySpark wurde installiert...\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting findspark\n",
            "  Downloading findspark-1.4.2-py2.py3-none-any.whl (4.2 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-1.4.2\n",
            "FindSpark wurde installiert...\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.0.tar.gz (281.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.3 MB 33 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.2\n",
            "  Downloading py4j-0.10.9.2-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 50.6 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.0-py2.py3-none-any.whl size=281805912 sha256=5575945594cc16930e0a9a5b95ae22577df6eafd6426ae3369a759e38852cacc\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/de/d2/9be5d59d7331c6c2a7c1b6d1a4f463ce107332b1ecd4e80718\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.2 pyspark-3.2.0\n",
            "PySpark wurde installiert...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9u5_oWDb7sw",
        "outputId": "73bf1511-ff1c-4d1e-8d7b-8413b3a84b14"
      },
      "source": [
        "# Initialisieren von findspark\n",
        "\n",
        "try: \n",
        "  import findspark\n",
        "  from pyspark.sql import SparkSession\n",
        "  \n",
        "  findspark.init()\n",
        "  \n",
        "  print(\"FindSpark und PySpark wurden initialisiert\")\n",
        "except ImportError: \n",
        "  raise ImportError(\"Fehler bei der Initialiserung von FindSpark und PySpark\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FindSpark und PySpark wurden initialisiert\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wjc0YuSDjZvn"
      },
      "source": [
        "# Einlesen und Vorbereiten der Textdatei\n",
        "\n",
        "Im ersten Abschnitt werden zunächst zwei Methoden definiert.\n",
        "\n",
        "* Der ersten Methode get_file_from_url werden als Parameter eine URL sowie ein Speicherort angegeben. Bei Ihrem Aufruf lädt die Methode eine Datei von der angegebenen URL herunter und speichert sie in Google Drive ab.\n",
        "\n",
        "* Die zweite Methode cut_file nimmt als Parameter einen numerischen Start- und Endwert sowie die Angabe einer Quell- und Zieldatei entgegen. Bei Ihrem Aufruf entfernt die Methode alle Zeilen vor bzw. nach den durch Start- und Endwert definierten Zeilenbereich aus der Quelldatei und speichert das Ergebnis in die Zieldatei.\n",
        "\n",
        "In dem folgenden Block wird dann im Anschluss die Datei mit den gesammelten Werken von Shakespeare von der Seite des MIT herunter geladen sowie von nicht benötigten Zeilen bereinigt und in einer neuen Datei gespeichert."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNaQNN9jkIie",
        "outputId": "d3449dbb-944e-4455-cb24-89346b954462"
      },
      "source": [
        "# Erstellen einer Methode, um Dateien aus dem Internet zu laden und zu speichern\n",
        "\n",
        "import requests \n",
        "\n",
        "def get_file_from_url(file_url, place_to_save):\n",
        "  try:\n",
        "    req = requests.get(file_url, stream = True) \n",
        "\n",
        "    with open(place_to_save, \"wb\") as file: \n",
        "\t    for block in req.iter_content(chunk_size = 1024): \n",
        "\t\t    if block: \n",
        "\t\t\t    file.write(block) \n",
        "     \n",
        "    print(\"Die Datei wurde herunter geladen und angelegt: {}\".format(file_url))\n",
        "  \n",
        "  except ValueError:\n",
        "    print(\"Fehler {}\".format(ValueError))   \n",
        "\n",
        "print(\"Die Funktion get_file_from_url wurde angelegt...\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Die Funktion get_file_from_url wurde angelegt...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf1_kePZkL4_",
        "outputId": "3f45c0b2-868c-45c3-cb85-93df9e5b8b5b"
      },
      "source": [
        "# Erstellen einer Methode, um eine Textdatei am Anfang und am Ende um die jeweils\n",
        "# angegebene Zahl an Reihen zu beschneiden.\n",
        "\n",
        "def cut_file(anfang, ende, quelldatei, zieldatei):\n",
        "  try:\n",
        "    with open(quelldatei, \"r\") as source:\n",
        "      lines = source.readlines()\n",
        "    \n",
        "    source.close()\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Start: {}\".format(anfang))\n",
        "    print(\"Ende: {}\".format(ende))\n",
        "    print(\"\")\n",
        "\n",
        "    current_count = 0\n",
        "  \n",
        "    with open(zieldatei, \"w\") as target:\n",
        "      for line in lines:\n",
        "        if current_count >= anfang and current_count <= ende:\n",
        "          target.write(line)\n",
        "\n",
        "        current_count = current_count + 1   \n",
        "    \n",
        "    target.close()\n",
        "\n",
        "    print(\"Datei wurde beschnitten...\")\n",
        "\n",
        "  except ValueError:\n",
        "    print(\"Fehler {}\".format(ValueError))\n",
        "\n",
        "print(\"Die Funktion cut_file wurde angelegt...\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Die Funktion cut_file wurde angelegt...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9qv_PqBlOVt",
        "outputId": "ee11fe1c-7b42-4ae6-9632-6275c7a11158"
      },
      "source": [
        "# Datei von der Quelle nach Colab laden\n",
        "\n",
        "file_url = \"https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt\"\n",
        "place_to_save = \"/content/shakespeare.txt\"\n",
        "\n",
        "get_file_from_url(file_url, place_to_save)\n",
        "\n",
        "print(\"\")\n",
        "print(\"Datei wurde vorbereitet...\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Die Datei wurde herunter geladen und angelegt: https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt\n",
            "\n",
            "Datei wurde vorbereitet...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HObiLObj8Yi",
        "outputId": "2782ef52-0818-497c-9e35-10cbb69a308b"
      },
      "source": [
        "# Unnötige Zeilen am Ende und am Start entfernen\n",
        "\n",
        "file_source = \"/content/shakespeare.txt\"\n",
        "file_target = \"/content/shakespeare_neu.txt\"\n",
        "\n",
        "cut_file(244,124438,file_source, file_target)\n",
        "\n",
        "print(\"\")\n",
        "print(\"Die Arbeitsdatei ist vorbereitet...\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Start: 244\n",
            "Ende: 124438\n",
            "\n",
            "Datei wurde beschnitten...\n",
            "\n",
            "Die Arbeitsdatei ist vorbereitet...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6HwX_ITMzWh"
      },
      "source": [
        "# Auszählen der Wörter\n",
        "\n",
        "Um mit Spark arbeiten zu können, muss als erstes eine Verbindung zu Spark in Form eines SparkContext aufgebaut werden. In dem hier verwendeten Code wird ein SparkContext erzeugt, welcher die Bezeichnung WordCounter erhält. Er soll lokal laufen und hierbei parallel alle verfügbaren Kerne verwenden. Dieser Block kann in einer Anwendung nur ein Mal ausgeführt werden.\n",
        "\n",
        "Anschließend wird die Textdatei eingelesen und gibt ein RDD in Form einer Liste von String zurück. In diesen Fall entsprechen die Strings den Zeilen der Textdatei. Die Methode map führt auf jedes Element des zugrunde liegenden RDD - also den Zeilen der Textdatei - die angegebene Funktion aus.\n",
        "\n",
        "In dem hier vorliegenden Fall findet zunächst eine Reihe von Ersetzungen (replace), dann eine Konvertierung in Kleinbuchstaben (lower) und am Schluss eine Filterung (filter) auf leere Zeilen statt. Als Ergebnis wird ein neues RDD vom Typ String zurückgegeben. Das ursprüngliche RDD wird nicht verändert. Es ist immutable. Die Verwendung einer FluentApi bewirkt eine übersichtliche Strukturierung des Codes.\n",
        "\n",
        "In der folgenden Codesequenze wird jedes Listenelement des RDD durch flatMap in seine einzelnen Wörter aufgeteilt. Für jedes Wort wird ein Tupel erzeugt und zurückgegeben. Da es sich um eine flatMap handelt, verfügt das zurück gegebene RDD nur noch über eine sehr lange Liste von Tupel. Die Funktion reduceByKey merged im Anschluss die einzelnen Tupel. Als Ergebnis erhält man eine Liste von Tupel mit eindeutigen Wörtern und deren Vorkommen.\n",
        "\n",
        "Mit der Methode sortBy wird auf die Anzahl der Wortvorkommen sortiert. Das zurück gegebene RDD sorted_counts kann im Anschluss ausgegeben werden, nachdem mit collect alle Werte eingesammelt wurden."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGSUceqCASSH",
        "outputId": "88050765-d154-4936-93ef-405834c35ade"
      },
      "source": [
        "# Erzeugen einer Spark Session\n",
        "\n",
        "session = SparkSession.builder.appName(\"Wordcount\").getOrCreate()\n",
        "\n",
        "print(\"Die Spark Session wurde angelegt...\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Die Spark Session wurde angelegt...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ikM4w-OlR7n",
        "outputId": "6ebae9e8-0fca-4a6e-ffb7-fa3f33c6fffa"
      },
      "source": [
        "# Auszählen der Wörter\n",
        "\n",
        "import pyspark.sql.functions as func\n",
        "\n",
        "dfx = session.read.text(file_target)\n",
        "\n",
        "top_out = 30\n",
        "top_length = 30\n",
        "\n",
        "print(\"\")\n",
        "print(\"Ausgabe der ersten {} Zeilen des Textes\".format(top_out))\n",
        "print(\"\")\n",
        "\n",
        "dfx.show(n=top_out,truncate=False)\n",
        "\n",
        "#dfx.printSchema()\n",
        "#dfx.describe().show()\n",
        "#print(dfx.columns)\n",
        "\n",
        "#https://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.functions.explode\n",
        "#https://dwgeek.com/replace-pyspark-dataframe-column-value-methods.html/\n",
        "\n",
        "dfx=dfx.withColumn('value', func.translate('value', ',', ' '))\n",
        "dfx=dfx.withColumn('value', func.translate('value', '.', ' '))\n",
        "dfx=dfx.withColumn('value', func.translate('value', '-', ' '))\n",
        "dfx=dfx.withColumn('value', func.lower('value'))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Ausgabe der {} größten Vorkommen\".format(top_length))\n",
        "print(\"\")\n",
        "\n",
        "dfx=dfx.withColumn('value2',func.explode(func.split(func.col('value'), ' ')))\\\n",
        "  .groupBy('value2')\\\n",
        "  .count()\\\n",
        "  .sort('count', ascending=False)\\\n",
        "  .show(n=top_length,truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ausgabe der ersten 30 Zeilen des Textes\n",
            "\n",
            "+-------------------------------------------------------+\n",
            "|value                                                  |\n",
            "+-------------------------------------------------------+\n",
            "|1609                                                   |\n",
            "|                                                       |\n",
            "|THE SONNETS                                            |\n",
            "|                                                       |\n",
            "|by William Shakespeare                                 |\n",
            "|                                                       |\n",
            "|                                                       |\n",
            "|                                                       |\n",
            "|                     1                                 |\n",
            "|  From fairest creatures we desire increase,           |\n",
            "|  That thereby beauty's rose might never die,          |\n",
            "|  But as the riper should by time decease,             |\n",
            "|  His tender heir might bear his memory:               |\n",
            "|  But thou contracted to thine own bright eyes,        |\n",
            "|  Feed'st thy light's flame with self-substantial fuel,|\n",
            "|  Making a famine where abundance lies,                |\n",
            "|  Thy self thy foe, to thy sweet self too cruel:       |\n",
            "|  Thou that art now the world's fresh ornament,        |\n",
            "|  And only herald to the gaudy spring,                 |\n",
            "|  Within thine own bud buriest thy content,            |\n",
            "|  And tender churl mak'st waste in niggarding:         |\n",
            "|    Pity the world, or else this glutton be,           |\n",
            "|    To eat the world's due, by the grave and thee.     |\n",
            "|                                                       |\n",
            "|                                                       |\n",
            "|                     2                                 |\n",
            "|  When forty winters shall besiege thy brow,           |\n",
            "|  And dig deep trenches in thy beauty's field,         |\n",
            "|  Thy youth's proud livery so gazed on now,            |\n",
            "|  Will be a tattered weed of small worth held:         |\n",
            "+-------------------------------------------------------+\n",
            "only showing top 30 rows\n",
            "\n",
            "\n",
            "Ausgabe der 30 größten Vorkommen\n",
            "\n",
            "+------+------+\n",
            "|value2|count |\n",
            "+------+------+\n",
            "|      |679850|\n",
            "|the   |27507 |\n",
            "|and   |26705 |\n",
            "|i     |20191 |\n",
            "|to    |19294 |\n",
            "|of    |18076 |\n",
            "|a     |14502 |\n",
            "|you   |12957 |\n",
            "|my    |12468 |\n",
            "|that  |10950 |\n",
            "|in    |10903 |\n",
            "|is    |9473  |\n",
            "|not   |8443  |\n",
            "|for   |8181  |\n",
            "|with  |7965  |\n",
            "|it    |7212  |\n",
            "|be    |6963  |\n",
            "|me    |6962  |\n",
            "|your  |6865  |\n",
            "|his   |6825  |\n",
            "|this  |6276  |\n",
            "|but   |6267  |\n",
            "|he    |6102  |\n",
            "|as    |5927  |\n",
            "|have  |5838  |\n",
            "|thou  |5378  |\n",
            "|so    |4948  |\n",
            "|will  |4858  |\n",
            "|him   |4625  |\n",
            "|by    |4386  |\n",
            "+------+------+\n",
            "only showing top 30 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovHhBQquBVwD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}